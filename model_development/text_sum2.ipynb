{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6043c-14c9-436b-a80f-9c8e2aa93de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:38:31.200248Z",
     "iopub.status.busy": "2025-03-22T13:38:31.200048Z",
     "iopub.status.idle": "2025-03-22T13:38:36.704574Z",
     "shell.execute_reply": "2025-03-22T13:38:36.703892Z",
     "shell.execute_reply.started": "2025-03-22T13:38:31.200231Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 13:38:34.581819: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-22 13:38:34.581894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-22 13:38:34.583212: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-22 13:38:34.590563: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-22 13:38:35.486682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig ,get_scheduler , AdamW\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from bitsandbytes.optim import Adam8bit\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from peft import PeftModel, LoraConfig, get_peft_model ,  prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df6f250-09e5-4839-8ccf-e0029268e3e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:38:36.705671Z",
     "iopub.status.busy": "2025-03-22T13:38:36.705350Z",
     "iopub.status.idle": "2025-03-22T13:38:36.763820Z",
     "shell.execute_reply": "2025-03-22T13:38:36.763172Z",
     "shell.execute_reply.started": "2025-03-22T13:38:36.705645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263e8cc-cfe4-42b8-be82-f731a03e29c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:38:36.770319Z",
     "iopub.status.busy": "2025-03-22T13:38:36.770168Z",
     "iopub.status.idle": "2025-03-22T13:38:43.975918Z",
     "shell.execute_reply": "2025-03-22T13:38:43.975396Z",
     "shell.execute_reply.started": "2025-03-22T13:38:36.770299Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,538,944 || all params: 585,940,224 || trainable%: 0.6039769681352343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MT5ForConditionalGeneration(\n",
       "      (shared): Embedding(250112, 768)\n",
       "      (encoder): MT5Stack(\n",
       "        (embed_tokens): Embedding(250112, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): MT5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): MT5LayerSelfAttention(\n",
       "                (SelfAttention): MT5Attention(\n",
       "                  (q): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): MT5LayerFF(\n",
       "                (DenseReluDense): MT5DenseGatedActDense(\n",
       "                  (wi_0): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x MT5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): MT5LayerSelfAttention(\n",
       "                (SelfAttention): MT5Attention(\n",
       "                  (q): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): MT5LayerFF(\n",
       "                (DenseReluDense): MT5DenseGatedActDense(\n",
       "                  (wi_0): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): MT5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): MT5Stack(\n",
       "        (embed_tokens): Embedding(250112, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): MT5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): MT5LayerSelfAttention(\n",
       "                (SelfAttention): MT5Attention(\n",
       "                  (q): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): MT5LayerCrossAttention(\n",
       "                (EncDecAttention): MT5Attention(\n",
       "                  (q): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): MT5LayerFF(\n",
       "                (DenseReluDense): MT5DenseGatedActDense(\n",
       "                  (wi_0): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x MT5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): MT5LayerSelfAttention(\n",
       "                (SelfAttention): MT5Attention(\n",
       "                  (q): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): MT5LayerCrossAttention(\n",
       "                (EncDecAttention): MT5Attention(\n",
       "                  (q): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): MT5LayerFF(\n",
       "                (DenseReluDense): MT5DenseGatedActDense(\n",
       "                  (wi_0): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): MT5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=250112, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig ,get_scheduler , AdamW\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import evaluate\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from bitsandbytes.optim import Adam8bit\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from peft import PeftModel, LoraConfig, get_peft_model ,  prepare_model_for_kbit_training\n",
    "import logging\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64 , \n",
    "    target_modules = [\"q\", \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"  \n",
    ")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  \n",
    "    llm_int8_threshold=6.0,  # determine threshold for dynamic quantization\n",
    "    device_map= {\"\": 0}\n",
    ")\n",
    "model_name = \"google/mt5-base\" # load model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # \n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, quantization_config=quantization_config,device_map={\"\": 0})\n",
    "\n",
    "model = prepare_model_for_kbit_training(base_model)\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff4a8a-1a72-4f76-953f-5eeb5a200719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:38:43.977688Z",
     "iopub.status.busy": "2025-03-22T13:38:43.977143Z",
     "iopub.status.idle": "2025-03-22T13:38:43.985379Z",
     "shell.execute_reply": "2025-03-22T13:38:43.984961Z",
     "shell.execute_reply.started": "2025-03-22T13:38:43.977667Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"model size after Apply Quantization: {model.get_memory_footprint() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61144fa-e74a-4f47-84d5-6232783c71f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:38:43.986491Z",
     "iopub.status.busy": "2025-03-22T13:38:43.986146Z",
     "iopub.status.idle": "2025-03-22T13:38:50.464480Z",
     "shell.execute_reply": "2025-03-22T13:38:50.463876Z",
     "shell.execute_reply.started": "2025-03-22T13:38:43.986472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    title  \\\n",
      "88734   ‡∏ü‡∏π‡πâ‡∏î‡∏ó‡∏£‡∏±‡∏Ñ‡πÑ‡∏ü‡∏£‡∏±‡πà‡∏ß ‡∏ä‡πá‡∏≠‡∏ï‡∏™‡∏≤‡∏ß‡∏™‡∏¥‡πâ‡∏ô‡πÉ‡∏à ‡∏Å‡∏ü‡∏ô.‡πÅ‡∏à‡∏á‡πÅ‡∏£‡∏á‡∏î‡∏±‡∏ô‡πÄ‡∏û‡∏µ‡∏¢...   \n",
      "333624  ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ‡πÇ‡∏ß‡∏¢ ‡∏Å‡∏ó‡∏Ñ.‡πÅ‡∏≠‡∏ö‡∏ó‡∏≥‡∏°‡∏ï‡∏¥‡∏•‡∏±‡∏ö ‡∏û‡∏£‡∏µ‡πÄ‡∏û‡∏î ‡πÄ‡∏≠‡∏∑‡πâ...   \n",
      "117808                          ‡∏´‡πâ‡∏≤‡∏°‡∏ß‡∏±‡∏î‡∏Å‡∏ó‡∏°.‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ñ‡∏≤‡∏ß‡∏£‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏   \n",
      "143066                 2‡∏ß‡∏≤‡∏¢‡∏£‡πâ‡∏≤‡∏¢‡∏ä‡∏¥‡∏á4.2‡πÅ‡∏™‡∏ô‡∏°‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏ö‡∏∏‡∏Å‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£   \n",
      "274333  ‡∏à‡∏≤‡∏¢ ‡∏ô‡∏≥‡∏ó‡∏±‡∏û‡∏™‡∏π‡πâ‡∏®‡∏∂‡∏Å‡∏ä‡∏¥‡∏á‡πÅ‡∏ä‡∏°‡∏õ‡πå‡πÄ‡∏≠‡πÄ‡∏ä‡∏µ‡∏¢ ‡∏•‡πà‡∏≤‡πÅ‡∏ï‡πâ‡∏°‡∏™‡∏∞‡∏™‡∏°‡πÇ‡∏≠‡∏•‡∏¥‡∏°‡∏õ‡∏¥‡∏Å   \n",
      "\n",
      "                                                     body  \\\n",
      "88734   ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ß‡∏•‡∏≤ 21.00 ‡∏ô.‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 2 ‡∏Å.‡∏Ñ.62 ‡∏û.‡∏ï.‡∏ó.‡∏£‡∏±‡∏Å‡πÄ‡∏Å‡∏µ...   \n",
      "333624   ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏±‡∏ö‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡πÇ‡∏ó‡∏£‡∏Ñ‡∏°‡∏ô‡∏≤‡∏Ñ‡∏°‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏°‡∏ï‡∏¥‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå...   \n",
      "117808  ‡πÄ‡∏à‡πâ‡∏≤‡∏Ñ‡∏ì‡∏∞‡πÉ‡∏´‡∏ç‡πà‡∏´‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡∏™‡∏±‡πà‡∏á ‡∏à‡∏ß‡∏Å‡∏Å‡∏≤‡∏ù‡∏≤‡∏Å‡∏ó‡πç‡∏≤‡∏•‡∏≤‡∏¢‡∏®‡∏≤‡∏™‡∏ô‡∏≤,‡πÄ‡∏à‡πâ‡∏≤...   \n",
      "143066  ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏ô‡∏µ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô‡πÅ‡∏ó‡πá‡∏Å‡∏ã‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏ö‡∏á‡∏Å‡πå ‡∏ï‡∏ö‡∏ö‡πà‡∏≤‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏û‡∏π...   \n",
      "274333   ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ 600-800 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÇ‡∏Ñ...   \n",
      "\n",
      "                                                  summary                type  \\\n",
      "88734   ‡πÄ‡∏£‡πà‡∏á‡∏´‡∏≤‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á ‡∏™‡∏≤‡∏ß‡∏ß‡∏±‡∏¢ 24 ‡∏õ‡∏µ ‡∏°‡∏≤‡∏ô‡∏±‡πà‡∏á‡∏Å‡∏¥‡∏ô‡∏Å...      ‡∏Ç‡πà‡∏≤‡∏ß,‡∏≠‡∏≤‡∏ä‡∏ç‡∏≤‡∏Å‡∏£‡∏£‡∏°   \n",
      "333624  ‡∏™‡∏´‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏ú‡∏π‡πâ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏£‡πâ‡∏≠‡∏á ‡∏Å‡∏ó‡∏Ñ.‡πÅ‡∏à‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏°‡∏µ...  ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï,‡πÑ‡∏≠‡∏ã‡∏µ‡∏ó‡∏µ   \n",
      "117808  ‡πÄ‡∏à‡πâ‡∏≤‡∏Ñ‡∏ì‡∏∞‡πÉ‡∏´‡∏ç‡πà‡∏´‡∏ô‡∏Å‡∏•‡∏≤‡∏á ‡∏™‡∏±‡πà‡∏á‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ñ‡∏≤‡∏ß‡∏£‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´...                None   \n",
      "143066  2 ‡πÇ‡∏à‡∏£‡∏ù‡∏±‡πà‡∏á‡∏ò‡∏ô‡∏Ø‡πÄ‡∏î‡∏¥‡∏ô‡∏°‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡πà‡∏≤‡∏ö‡∏∏‡∏Å‡πÄ‡∏Ñ‡∏≤‡∏ô‡πå‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ò‡∏ô‡∏ä...        ‡∏Ç‡πà‡∏≤‡∏ß,‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏ó‡∏¢   \n",
      "274333  ‡∏à‡∏≤‡∏¢ ‡∏≠‡∏±‡∏á‡∏Ñ‡πå‡∏™‡∏∏‡∏ó‡∏ò‡∏≤‡∏™‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡πå ‡∏ô‡∏≥‡∏ó‡∏±‡∏û‡∏ô‡∏±‡∏Å‡∏õ‡∏±‡πà‡∏ô‡πÑ‡∏ó‡∏¢ ‡∏•‡∏∏‡∏¢‡∏®‡∏∂‡∏Å ‡πÄ...      ‡∏Å‡∏µ‡∏¨‡∏≤,‡∏Å‡∏µ‡∏¨‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÜ   \n",
      "\n",
      "                                                     tags  \\\n",
      "88734     ‡πÑ‡∏ü‡∏£‡∏±‡πà‡∏ß,‡πÑ‡∏ü‡∏ä‡πá‡∏≠‡∏ï,‡∏ü‡∏π‡πâ‡∏î‡∏ó‡∏£‡∏±‡∏Ñ,‡∏ö‡∏≤‡∏á‡πÉ‡∏´‡∏ç‡πà,‡πÑ‡∏ü‡∏î‡∏π‡∏î,‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ   \n",
      "333624  ‡∏Å‡∏ó‡∏Ñ.,‡∏Å‡∏™‡∏ó‡∏ä.,‡∏ö‡∏∏‡∏ç‡∏¢‡∏∑‡∏ô ‡∏®‡∏¥‡∏£‡∏¥‡∏ò‡∏£‡∏£‡∏°,‡∏û‡∏£‡∏µ‡πÄ‡∏û‡∏î,‡∏™‡∏´‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏≠‡∏á‡∏Ñ‡πå‡∏Å...   \n",
      "117808  ‡πÄ‡∏à‡πâ‡∏≤‡∏Ñ‡∏ì‡∏∞‡πÉ‡∏´‡∏ç‡πà‡∏´‡∏ô‡∏Å‡∏•‡∏≤‡∏á,‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ñ‡∏≤‡∏ß‡∏£‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏,‡πÄ‡∏à‡πâ‡∏≤‡∏Ñ‡∏ì‡∏∞‡∏Å‡∏£...   \n",
      "143066  ‡∏Ç‡πà‡∏≤‡∏ß‡∏´‡∏ô‡πâ‡∏≤1,‡∏õ‡∏•‡πâ‡∏ô‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£,‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ò‡∏ô‡∏ä‡∏≤‡∏ï,‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏° 2,‡∏ä‡∏¥‡∏á‡πÄ...   \n",
      "274333  ‡∏à‡∏≤‡∏¢ ‡∏≠‡∏±‡∏á‡∏Ñ‡πå‡∏™‡∏∏‡∏ó‡∏ò‡∏≤‡∏™‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡πå,‡∏ô‡∏±‡∏Å‡∏à‡∏±‡∏Å‡∏£‡∏¢‡∏≤‡∏ô‡∏ó‡∏µ‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡πÑ‡∏ó‡∏¢,‡∏à‡∏±‡∏Å...   \n",
      "\n",
      "                                                    url  \n",
      "88734     https://www.thairath.co.th/news/crime/1605725  \n",
      "333624      https://prachatai.com/journal/2013/02/45267  \n",
      "117808        https://www.thairath.co.th/content/612356  \n",
      "143066     https://www.thairath.co.th/news/local/497257  \n",
      "274333  https://www.thairath.co.th/sport/others/1677385  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#load and prepare data\n",
    "def load_and_prepare_data():\n",
    "    dataset = load_dataset(\"pythainlp/thaisum\")\n",
    "\n",
    "    df_train_50k = dataset[\"train\"].to_pandas().sample(n=50000, random_state=42) # use only 50,000 samples\n",
    "\n",
    "    df1_train = df_train_50k.sample(n=6700, random_state=42) # only 6700 because of limitation of VRAM\n",
    "    df_remaining = df_train_50k.drop(df1_train.index)\n",
    "\n",
    "    df_train = df1_train\n",
    "    df_valid = dataset[\"validation\"].to_pandas()  # validation set\n",
    "    df_test = dataset[\"test\"].to_pandas()\n",
    "\n",
    "\n",
    "    print(df_train.head())\n",
    "    valid_size = int(df_train.shape[0] * 0.1)  # use 10% of train set\n",
    "    test_size = int(df_train.shape[0] * 0.1)\n",
    "\n",
    "    # \n",
    "    df_valid = df_valid.sample(n=valid_size, random_state=42)\n",
    "    df_test = df_test.sample(n=test_size, random_state=42)\n",
    "\n",
    "    return df_train, df_valid, df_test\n",
    "\n",
    "class TextSummaryDataset(Dataset):\n",
    "    def __init__(self, texts, summaries, tokenizer, max_length=1024):\n",
    "    # def __init__(self, texts, summaries, tokenizer, max_length=1024):\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = str(self.texts[idx])\n",
    "        target_text = str(self.summaries[idx])\n",
    "\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = target_encoding.input_ids.squeeze(0) # Ignore padding during computation.\n",
    "        labels[labels == 0] = -100 \n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encoding.input_ids.squeeze(0),\n",
    "            \"attention_mask\": input_encoding.attention_mask.squeeze(0),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "df_train, df_valid, df_test = load_and_prepare_data()\n",
    "\n",
    "# build dataset\n",
    "train_dataset = TextSummaryDataset(df_train[\"body\"].tolist(), df_train[\"summary\"].tolist(), tokenizer)\n",
    "valid_dataset = TextSummaryDataset(df_valid[\"body\"].tolist(), df_valid[\"summary\"].tolist(), tokenizer)\n",
    "test_dataset = TextSummaryDataset(df_test[\"body\"].tolist(), df_test[\"summary\"].tolist(), tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d102701-e274-4b2a-a8c5-916ee583e83c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:41:35.902162Z",
     "iopub.status.busy": "2025-03-22T13:41:35.901566Z",
     "iopub.status.idle": "2025-03-22T13:41:36.872812Z",
     "shell.execute_reply": "2025-03-22T13:41:36.871226Z",
     "shell.execute_reply.started": "2025-03-22T13:41:35.902131Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results', \n",
    "    num_train_epochs=18, \n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    learning_rate=5e-5,\n",
    "    eval_accumulation_steps=2,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"steps\",  \n",
    "    save_strategy=\"steps\", \n",
    "    warmup_steps = 500, \n",
    "    eval_steps=100,\n",
    "    save_steps=100,  \n",
    "    save_total_limit=2, \n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",\n",
    "    bf16=True,\n",
    "    lr_scheduler_type='cosine_with_restarts', # avoid local minima\n",
    "    predict_with_generate=True,\n",
    "    optim=\"paged_adamw_8bit\",  # Using AdamW optimizer from torch\n",
    "    load_best_model_at_end=True, \n",
    "    gradient_accumulation_steps = 8, # reduce vram\n",
    "\n",
    "\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Train model \n",
    "for name, param in model.named_parameters():\n",
    "    if not param.is_contiguous():\n",
    "        param.data = param.data.contiguous()\n",
    "\n",
    "project = \"mt5base-thesis-finetune-\"\n",
    "run_name = 'train-model-dir-round-5'\n",
    "output_dir = \"./\" + project + run_name\n",
    "\n",
    "trainer.train()\n",
    "model.config._name_or_path = output_dir\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "base_model = model.base_model  \n",
    "base_model.save_pretrained(output_dir)  \n",
    "trainer.save_model(output_dir) \n",
    "model.config.save_pretrained(output_dir)  \n",
    "base_model.config.save_pretrained(output_dir)  \n",
    "\n",
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb31072-54b8-4f02-8162-3e20422029f6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-22T13:38:52.687862Z",
     "iopub.status.idle": "2025-03-22T13:38:52.688022Z",
     "shell.execute_reply": "2025-03-22T13:38:52.687940Z",
     "shell.execute_reply.started": "2025-03-22T13:38:52.687940Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"./mt5base-thesis-finetune-train-model-dir-round-5\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e03707-3229-4a58-8312-95171bf985a6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-22T13:38:52.688811Z",
     "iopub.status.idle": "2025-03-22T13:38:52.688987Z",
     "shell.execute_reply": "2025-03-22T13:38:52.688902Z",
     "shell.execute_reply.started": "2025-03-22T13:38:52.688894Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#  ROUGE score calculation\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, num_samples):\n",
    "    model.eval() \n",
    "    preds, refs = [], []\n",
    "    \n",
    "    for i, sample in enumerate(dataset):\n",
    "        if i >= num_samples: \n",
    "            break\n",
    "        \n",
    "        input_text = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n",
    "        reference_summary = tokenizer.decode(\n",
    "            [token for token in sample[\"labels\"] if token != -100], skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        inputs = {\n",
    "            \"input_ids\": sample[\"input_ids\"].unsqueeze(0).to(device),\n",
    "            \"attention_mask\": sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "        }\n",
    "        \n",
    "        # Generate summary with optimized parameters\n",
    "        with torch.no_grad():output_tokens = model.generate(**inputs, max_length=1000, num_beams=5)\n",
    "        \n",
    "        # Decode summary output and append\n",
    "        predicted_summary = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        preds.append(predicted_summary)\n",
    "        refs.append(reference_summary)\n",
    "    \n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ROUGE score\n",
    "    rouge_scores = rouge.compute(predictions=preds, references=refs)\n",
    "    \n",
    "    return rouge_scores\n",
    "\n",
    "# ROUGE Score\n",
    "rouge_results = evaluate_model(model, tokenizer, valid_dataset, num_samples=200)\n",
    "\n",
    "print(\"ROUGE Score:\", rouge_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a4bce2-fe55-424b-9e8d-d58dc22f780a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-22T13:38:52.690014Z",
     "iopub.status.idle": "2025-03-22T13:38:52.690249Z",
     "shell.execute_reply": "2025-03-22T13:38:52.690096Z",
     "shell.execute_reply.started": "2025-03-22T13:38:52.690096Z"
    }
   },
   "outputs": [],
   "source": [
    "# real test \n",
    "def summarize_text(text, model, tokenizer, max_input_length=3000, max_output_length=1500):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_input_length)\n",
    "    \n",
    "    # Move tensors to the same device as the model\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=max_output_length,\n",
    "    min_length=200,\n",
    "    num_beams=5,             \n",
    "    do_sample=True,        \n",
    "    length_penalty=1.1,      \n",
    "    no_repeat_ngram_size=3,  \n",
    "    repetition_penalty=1.1,   \n",
    "    top_p=0.95,               \n",
    "    temperature=0.9,\n",
    "    early_stopping=True       \n",
    "    )\n",
    "\n",
    "\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "sample_text = \"\"\"‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢- ‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡πà‡∏≤ ‡∏£‡∏≤‡∏ä‡∏≠‡∏≤‡∏ì‡∏≤‡∏à‡∏±‡∏Å‡∏£‡πÑ‡∏ó‡∏¢ ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏±‡∏ê‡∏ä‡∏≤‡∏ï‡∏¥‡∏≠‡∏±‡∏ô‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô‡∏Ñ‡∏≤‡∏ö‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏≠‡∏¥‡∏ô‡πÇ‡∏î‡∏à‡∏µ‡∏ô‡πÅ‡∏•‡∏∞‡∏°‡∏•‡∏≤‡∏¢‡∏π ‡πÉ‡∏ô‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ‡πÄ‡∏≠‡πÄ‡∏ä‡∏µ‡∏¢‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡πÄ‡∏â‡∏µ‡∏¢‡∏á‡πÉ‡∏ï‡πâ ‡∏°‡∏µ‡∏û‡∏£‡∏°‡πÅ‡∏î‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏≠‡∏≠‡∏Å‡∏ï‡∏¥‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏•‡∏≤‡∏ß‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏Å‡∏±‡∏°‡∏û‡∏π‡∏ä‡∏≤ ‡∏ó‡∏¥‡∏®‡πÉ‡∏ï‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏î‡∏ô‡∏ï‡πà‡∏≠‡πÅ‡∏î‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏°‡∏≤‡πÄ‡∏•‡πÄ‡∏ã‡∏µ‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡πà‡∏≤‡∏ß‡πÑ‡∏ó‡∏¢ ‡∏ó‡∏¥‡∏®‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏ï‡∏Å‡∏ï‡∏¥‡∏î‡∏ó‡∏∞‡πÄ‡∏•‡∏≠‡∏±‡∏ô‡∏î‡∏≤‡∏°‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏û‡∏°‡πà‡∏≤ ‡πÅ‡∏•‡∏∞‡∏ó‡∏¥‡∏®‡πÄ‡∏´‡∏ô‡∏∑‡∏≠‡∏ï‡∏¥‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏û‡∏°‡πà‡∏≤‡πÅ‡∏•‡∏∞‡∏•‡∏≤‡∏ß ‡∏°‡∏µ‡πÅ‡∏°‡πà‡∏ô‡πâ‡∏≥‡πÇ‡∏Ç‡∏á‡∏Å‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏≤‡∏á‡∏ä‡πà‡∏ß‡∏á ‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ò‡∏¥‡∏õ‡πÑ‡∏ï‡∏¢‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏£‡∏±‡∏ê‡∏™‡∏†‡∏≤ ‡∏°‡∏µ‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏Å‡∏•‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£ ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ ‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡πÄ‡∏õ‡πá‡∏ô 76 ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î\n",
    "‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 50 ‡∏Ç‡∏≠‡∏á‡πÇ‡∏•‡∏Å ‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà 513,115 ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Å‡∏¥‡πÇ‡∏•‡πÄ‡∏°‡∏ï‡∏£ ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡∏°‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 20 ‡∏Ç‡∏≠‡∏á‡πÇ‡∏•‡∏Å ‡∏Ñ‡∏∑‡∏≠ ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 66 ‡∏•‡πâ‡∏≤‡∏ô‡∏Ñ‡∏ô ‡∏Å‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏´‡∏°‡πà ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏Ñ‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ ‡πÑ‡∏ó‡∏¢‡∏°‡∏µ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡πà‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏°‡∏≤‡∏Å ‡∏≠‡∏≤‡∏ó‡∏¥ ‡∏û‡∏±‡∏ó‡∏¢‡∏≤, ‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï, ‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£ ‡πÅ‡∏•‡∏∞‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡πà‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏≠‡∏±‡∏ô‡∏°‡∏µ‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏µ‡∏î‡∏µ‡∏û‡∏µ‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏® ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡∏£‡∏≤‡∏ß 334,026 ‡∏•‡πâ‡∏≤‡∏ô‡∏î‡∏≠‡∏•‡∏•‡∏≤‡∏£‡πå‡∏™‡∏´‡∏£‡∏±‡∏ê ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡πÉ‡∏ô ‡∏û.‡∏®. 2553 ‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏ô‡∏±‡∏ö‡∏ß‡πà‡∏≤‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 30 ‡∏Ç‡∏≠‡∏á‡πÇ‡∏•‡∏Å\n",
    "‡πÉ‡∏ô‡∏≠‡∏≤‡∏ì‡∏≤‡πÄ‡∏Ç‡∏ï‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡∏û‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡∏≠‡∏≤‡∏¢‡∏∏‡πÄ‡∏Å‡πà‡∏≤‡πÅ‡∏Å‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ñ‡∏∂‡∏á‡∏´‡πâ‡∏≤‡πÅ‡∏™‡∏ô‡∏õ‡∏µ ‡∏ô‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏°‡∏±‡∏Å‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏≠‡∏≤‡∏ì‡∏≤‡∏à‡∏±‡∏Å‡∏£‡∏™‡∏∏‡πÇ‡∏Ç‡∏ó‡∏±‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÑ‡∏ó‡∏¢ ‡∏ã‡∏∂‡πà‡∏á‡∏ï‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏Å‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏≠‡∏¥‡∏ó‡∏ò‡∏¥‡∏û‡∏•‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏ì‡∏≤‡∏à‡∏±‡∏Å‡∏£‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤ ‡∏≠‡∏±‡∏ô‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö‡∏ä‡∏≤‡∏ï‡∏¥‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏ï‡∏Å ‡∏≠‡∏≤‡∏ì‡∏≤‡∏à‡∏±‡∏Å‡∏£‡∏≠‡∏¢‡∏∏‡∏ò‡∏¢‡∏≤‡∏°‡∏µ‡∏≠‡∏≤‡∏¢‡∏∏‡∏¢‡∏∑‡∏ô‡∏¢‡∏≤‡∏ß 417 ‡∏õ‡∏µ‡∏Å‡πá‡πÄ‡∏™‡∏∑‡πà‡∏≠‡∏°‡∏≠‡∏≥‡∏ô‡∏≤‡∏à‡πÅ‡∏•‡∏∞‡∏•‡πà‡∏°‡∏™‡∏•‡∏≤‡∏¢‡πÑ‡∏õ‡πÇ‡∏î‡∏¢‡∏™‡∏¥‡πâ‡∏ô‡πÄ‡∏ä‡∏¥‡∏á ‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡πÄ‡∏à‡πâ‡∏≤‡∏Å‡∏£‡∏∏‡∏á‡∏ò‡∏ô‡∏ö‡∏∏‡∏£‡∏µ‡∏ó‡∏£‡∏á‡∏Å‡∏≠‡∏ö‡∏Å‡∏π‡πâ‡πÄ‡∏≠‡∏Å‡∏£‡∏≤‡∏ä‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏õ‡∏ô‡∏≤‡∏≠‡∏≤‡∏ì‡∏≤‡∏à‡∏±‡∏Å‡∏£‡∏ò‡∏ô‡∏ö‡∏∏‡∏£‡∏µ ‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡∏∏‡πà‡∏ô‡∏ß‡∏≤‡∏¢‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏•‡∏≤‡∏¢‡∏≠‡∏≤‡∏ì‡∏≤‡∏à‡∏±‡∏Å‡∏£ ‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏¢‡∏∏‡∏Ñ‡∏™‡∏°‡∏±‡∏¢‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏ä‡∏ß‡∏á‡∏®‡πå‡∏à‡∏±‡∏Å‡∏£‡∏µ‡πÅ‡∏´‡πà‡∏á‡∏Å‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏ï‡∏ô‡πÇ‡∏Å‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡πå\n",
    "‡∏ä‡πà‡∏ß‡∏á‡∏ï‡πâ‡∏ô‡∏Å‡∏£‡∏∏‡∏á ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÄ‡∏ú‡∏ä‡∏¥‡∏ç‡∏†‡∏±‡∏¢‡∏Ñ‡∏∏‡∏Å‡∏Ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ä‡∏≤‡∏ï‡∏¥‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á ‡πÅ‡∏ï‡πà‡∏´‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ä‡∏™‡∏°‡∏±‡∏¢‡∏û‡∏£‡∏∞‡∏ö‡∏≤‡∏ó‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à‡∏û‡∏£‡∏∞‡∏à‡∏≠‡∏°‡πÄ‡∏Å‡∏•‡πâ‡∏≤‡πÄ‡∏à‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏±‡∏ß‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡∏°‡∏≤ ‡∏ä‡∏≤‡∏ï‡∏¥‡∏ï‡∏∞‡∏ß‡∏±‡∏ô‡∏ï‡∏Å‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏µ‡∏≠‡∏¥‡∏ó‡∏ò‡∏¥‡∏û‡∏•‡πÉ‡∏ô‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å ‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏Ñ‡∏µ‡πÅ‡∏´‡πà‡∏á‡∏™‡∏ô‡∏ò‡∏¥‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏´‡∏•‡∏≤‡∏¢‡∏â‡∏ö‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏µ‡∏¢‡∏î‡∏¥‡∏ô‡πÅ‡∏î‡∏ô‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô ‡∏Å‡∏£‡∏∞‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏ó‡∏¢‡∏Å‡πá‡∏¢‡∏±‡∏á‡∏ò‡∏≥‡∏£‡∏á‡∏ï‡∏ô‡∏°‡∏¥‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏≤‡∏ì‡∏≤‡∏ô‡∏¥‡∏Ñ‡∏°‡∏Ç‡∏≠‡∏á‡∏ä‡∏≤‡∏ï‡∏¥‡πÉ‡∏î ‡πÜ ‡∏ï‡πà‡∏≠‡∏°‡∏≤‡∏à‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÇ‡∏•‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏ù‡πà‡∏≤‡∏¢‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡∏°‡∏¥‡∏ï‡∏£ ‡πÅ‡∏•‡∏∞‡πÉ‡∏ô‡∏õ‡∏µ ‡∏û.‡∏®. 2475 ‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ß‡∏±‡∏ï‡∏¥‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏≠‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏ç‡∏≤‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏£‡∏≤‡∏ä‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ò‡∏¥‡∏õ‡πÑ‡∏ï‡∏¢ ‡πÅ‡∏•‡∏∞‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏ù‡πà‡∏≤‡∏¢‡∏≠‡∏±‡∏Å‡∏©‡∏∞‡πÉ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÇ‡∏•‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á ‡∏à‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏™‡∏á‡∏Ñ‡∏£‡∏≤‡∏°‡πÄ‡∏¢‡πá‡∏ô ‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏±‡∏ô‡∏ò‡∏°‡∏¥‡∏ï‡∏£‡∏Å‡∏±‡∏ö‡∏™‡∏´‡∏£‡∏±‡∏ê‡∏≠‡πÄ‡∏°‡∏£‡∏¥‡∏Å‡∏≤ ‡∏ó‡∏´‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏°‡∏µ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡πÑ‡∏ó‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏è‡∏¥‡∏ß‡∏±‡∏ï‡∏¥‡∏™‡∏¢‡∏≤‡∏°‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏•‡∏≤‡∏¢‡∏™‡∏¥‡∏ö‡∏õ‡∏µ ‡∏Å‡∏£‡∏∞‡∏ó‡∏±‡πà‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡∏û‡∏•‡πÄ‡∏£‡∏∑‡∏≠‡∏ô ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏¢‡∏∏‡∏Ñ‡πÇ‡∏•‡∏Å‡πÄ‡∏™‡∏£‡∏µ‡πÉ‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô\"\"\"\n",
    "print(\"üîπ ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö: \", sample_text)\n",
    "print(\"üîπ ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ:\", summarize_text(sample_text, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f758f92-cd64-4ad3-8b30-44cad1266e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
