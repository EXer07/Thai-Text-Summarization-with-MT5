{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6043c-14c9-436b-a80f-9c8e2aa93de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:38:31.200248Z",
     "iopub.status.busy": "2025-03-22T13:38:31.200048Z",
     "iopub.status.idle": "2025-03-22T13:38:36.704574Z",
     "shell.execute_reply": "2025-03-22T13:38:36.703892Z",
     "shell.execute_reply.started": "2025-03-22T13:38:31.200231Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 13:38:34.581819: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-22 13:38:34.581894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-22 13:38:34.583212: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-22 13:38:34.590563: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-22 13:38:35.486682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig ,get_scheduler , AdamW\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from bitsandbytes.optim import Adam8bit\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from peft import PeftModel, LoraConfig, get_peft_model ,  prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df6f250-09e5-4839-8ccf-e0029268e3e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:38:36.705671Z",
     "iopub.status.busy": "2025-03-22T13:38:36.705350Z",
     "iopub.status.idle": "2025-03-22T13:38:36.763820Z",
     "shell.execute_reply": "2025-03-22T13:38:36.763172Z",
     "shell.execute_reply.started": "2025-03-22T13:38:36.705645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263e8cc-cfe4-42b8-be82-f731a03e29c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:38:36.770319Z",
     "iopub.status.busy": "2025-03-22T13:38:36.770168Z",
     "iopub.status.idle": "2025-03-22T13:38:43.975918Z",
     "shell.execute_reply": "2025-03-22T13:38:43.975396Z",
     "shell.execute_reply.started": "2025-03-22T13:38:36.770299Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,538,944 || all params: 585,940,224 || trainable%: 0.6039769681352343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MT5ForConditionalGeneration(\n",
       "      (shared): Embedding(250112, 768)\n",
       "      (encoder): MT5Stack(\n",
       "        (embed_tokens): Embedding(250112, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): MT5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): MT5LayerSelfAttention(\n",
       "                (SelfAttention): MT5Attention(\n",
       "                  (q): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): MT5LayerFF(\n",
       "                (DenseReluDense): MT5DenseGatedActDense(\n",
       "                  (wi_0): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x MT5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): MT5LayerSelfAttention(\n",
       "                (SelfAttention): MT5Attention(\n",
       "                  (q): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): MT5LayerFF(\n",
       "                (DenseReluDense): MT5DenseGatedActDense(\n",
       "                  (wi_0): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): MT5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): MT5Stack(\n",
       "        (embed_tokens): Embedding(250112, 768)\n",
       "        (block): ModuleList(\n",
       "          (0): MT5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): MT5LayerSelfAttention(\n",
       "                (SelfAttention): MT5Attention(\n",
       "                  (q): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 12)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): MT5LayerCrossAttention(\n",
       "                (EncDecAttention): MT5Attention(\n",
       "                  (q): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): MT5LayerFF(\n",
       "                (DenseReluDense): MT5DenseGatedActDense(\n",
       "                  (wi_0): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-11): 11 x MT5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): MT5LayerSelfAttention(\n",
       "                (SelfAttention): MT5Attention(\n",
       "                  (q): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): MT5LayerCrossAttention(\n",
       "                (EncDecAttention): MT5Attention(\n",
       "                  (q): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (k): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  (v): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (o): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): MT5LayerFF(\n",
       "                (DenseReluDense): MT5DenseGatedActDense(\n",
       "                  (wi_0): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wi_1): Linear4bit(in_features=768, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): NewGELUActivation()\n",
       "                )\n",
       "                (layer_norm): MT5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): MT5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=250112, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig ,get_scheduler , AdamW\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import evaluate\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from bitsandbytes.optim import Adam8bit\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from peft import PeftModel, LoraConfig, get_peft_model ,  prepare_model_for_kbit_training\n",
    "import logging\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64 , \n",
    "    target_modules = [\"q\", \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"  \n",
    ")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  \n",
    "    llm_int8_threshold=6.0,  # determine threshold for dynamic quantization\n",
    "    device_map= {\"\": 0}\n",
    ")\n",
    "model_name = \"google/mt5-base\" # load model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # \n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, quantization_config=quantization_config,device_map={\"\": 0})\n",
    "\n",
    "model = prepare_model_for_kbit_training(base_model)\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff4a8a-1a72-4f76-953f-5eeb5a200719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:38:43.977688Z",
     "iopub.status.busy": "2025-03-22T13:38:43.977143Z",
     "iopub.status.idle": "2025-03-22T13:38:43.985379Z",
     "shell.execute_reply": "2025-03-22T13:38:43.984961Z",
     "shell.execute_reply.started": "2025-03-22T13:38:43.977667Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"model size after Apply Quantization: {model.get_memory_footprint() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61144fa-e74a-4f47-84d5-6232783c71f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:38:43.986491Z",
     "iopub.status.busy": "2025-03-22T13:38:43.986146Z",
     "iopub.status.idle": "2025-03-22T13:38:50.464480Z",
     "shell.execute_reply": "2025-03-22T13:38:50.463876Z",
     "shell.execute_reply.started": "2025-03-22T13:38:43.986472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    title  \\\n",
      "88734   ฟู้ดทรัคไฟรั่ว ช็อตสาวสิ้นใจ กฟน.แจงแรงดันเพีย...   \n",
      "333624  องค์กรผู้บริโภคโวย กทค.แอบทำมติลับ พรีเพด เอื้...   \n",
      "117808                          ห้ามวัดกทม.สร้างถาวรวัตถุ   \n",
      "143066                 2วายร้ายชิง4.2แสนมือเปล่าบุกธนาคาร   \n",
      "274333  จาย นำทัพสู้ศึกชิงแชมป์เอเชีย ล่าแต้มสะสมโอลิมปิก   \n",
      "\n",
      "                                                     body  \\\n",
      "88734   เมื่อเวลา 21.00 น.วันที่ 2 ก.ค.62 พ.ต.ท.รักเกี...   \n",
      "333624   พร้อมสับองค์กรโทรคมนาคมเผยแพร่มติผ่านเว็บไซต์...   \n",
      "117808  เจ้าคณะใหญ่หนกลางสั่ง จวกกาฝากทําลายศาสนา,เจ้า...   \n",
      "143066  แล้วหนีออกมาขึ้นแท็กซี่หน้าแบงก์ ตบบ่าลูกค้าพู...   \n",
      "274333   หากเก็บเพิ่มได้ 600-800 คะแนน โอกาสที่จะได้โค...   \n",
      "\n",
      "                                                  summary                type  \\\n",
      "88734   เร่งหาสาเหตุที่แท้จริง สาววัย 24 ปี มานั่งกินก...      ข่าว,อาชญากรรม   \n",
      "333624  สหพันธ์องค์กรผู้บริโภค เรียกร้อง กทค.แจงเหตุมี...  คุณภาพชีวิต,ไอซีที   \n",
      "117808  เจ้าคณะใหญ่หนกลาง สั่งห้ามสร้างถาวรวัตถุเพื่อห...                None   \n",
      "143066  2 โจรฝั่งธนฯเดินมือเปล่าบุกเคาน์เตอร์ธนาคารธนช...        ข่าว,ทั่วไทย   \n",
      "274333  จาย อังค์สุทธาสาวิทย์ นำทัพนักปั่นไทย ลุยศึก เ...      กีฬา,กีฬาอื่นๆ   \n",
      "\n",
      "                                                     tags  \\\n",
      "88734     ไฟรั่ว,ไฟช็อต,ฟู้ดทรัค,บางใหญ่,ไฟดูด,ข่าวทั่วไป   \n",
      "333624  กทค.,กสทช.,บุญยืน ศิริธรรม,พรีเพด,สหพันธ์องค์ก...   \n",
      "117808  เจ้าคณะใหญ่หนกลาง,ห้ามสร้างถาวรวัตถุ,เจ้าคณะกร...   \n",
      "143066  ข่าวหน้า1,ปล้นธนาคาร,ธนาคารธนชาต,พระราม 2,ชิงเ...   \n",
      "274333  จาย อังค์สุทธาสาวิทย์,นักจักรยานทีมชาติไทย,จัก...   \n",
      "\n",
      "                                                    url  \n",
      "88734     https://www.thairath.co.th/news/crime/1605725  \n",
      "333624      https://prachatai.com/journal/2013/02/45267  \n",
      "117808        https://www.thairath.co.th/content/612356  \n",
      "143066     https://www.thairath.co.th/news/local/497257  \n",
      "274333  https://www.thairath.co.th/sport/others/1677385  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#load and prepare data\n",
    "def load_and_prepare_data():\n",
    "    dataset = load_dataset(\"pythainlp/thaisum\")\n",
    "\n",
    "    df_train_50k = dataset[\"train\"].to_pandas().sample(n=50000, random_state=42) # use only 50,000 samples\n",
    "\n",
    "    df1_train = df_train_50k.sample(n=6700, random_state=42) # only 6700 because of limitation of VRAM\n",
    "    df_remaining = df_train_50k.drop(df1_train.index)\n",
    "\n",
    "    df_train = df1_train\n",
    "    df_valid = dataset[\"validation\"].to_pandas()  # validation set\n",
    "    df_test = dataset[\"test\"].to_pandas()\n",
    "\n",
    "\n",
    "    print(df_train.head())\n",
    "    valid_size = int(df_train.shape[0] * 0.1)  # use 10% of train set\n",
    "    test_size = int(df_train.shape[0] * 0.1)\n",
    "\n",
    "    # \n",
    "    df_valid = df_valid.sample(n=valid_size, random_state=42)\n",
    "    df_test = df_test.sample(n=test_size, random_state=42)\n",
    "\n",
    "    return df_train, df_valid, df_test\n",
    "\n",
    "class TextSummaryDataset(Dataset):\n",
    "    def __init__(self, texts, summaries, tokenizer, max_length=1024):\n",
    "    # def __init__(self, texts, summaries, tokenizer, max_length=1024):\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = str(self.texts[idx])\n",
    "        target_text = str(self.summaries[idx])\n",
    "\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = target_encoding.input_ids.squeeze(0) # Ignore padding during computation.\n",
    "        labels[labels == 0] = -100 \n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encoding.input_ids.squeeze(0),\n",
    "            \"attention_mask\": input_encoding.attention_mask.squeeze(0),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "df_train, df_valid, df_test = load_and_prepare_data()\n",
    "\n",
    "# build dataset\n",
    "train_dataset = TextSummaryDataset(df_train[\"body\"].tolist(), df_train[\"summary\"].tolist(), tokenizer)\n",
    "valid_dataset = TextSummaryDataset(df_valid[\"body\"].tolist(), df_valid[\"summary\"].tolist(), tokenizer)\n",
    "test_dataset = TextSummaryDataset(df_test[\"body\"].tolist(), df_test[\"summary\"].tolist(), tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d102701-e274-4b2a-a8c5-916ee583e83c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-22T13:41:35.902162Z",
     "iopub.status.busy": "2025-03-22T13:41:35.901566Z",
     "iopub.status.idle": "2025-03-22T13:41:36.872812Z",
     "shell.execute_reply": "2025-03-22T13:41:36.871226Z",
     "shell.execute_reply.started": "2025-03-22T13:41:35.902131Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# สร้าง training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results', \n",
    "    num_train_epochs=18, \n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    learning_rate=5e-5,\n",
    "    eval_accumulation_steps=2,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"steps\",  \n",
    "    save_strategy=\"steps\", \n",
    "    warmup_steps = 500, \n",
    "    eval_steps=100,\n",
    "    save_steps=100,  \n",
    "    save_total_limit=2, \n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",\n",
    "    bf16=True,\n",
    "    lr_scheduler_type='cosine_with_restarts', # avoid local minima\n",
    "    predict_with_generate=True,\n",
    "    optim=\"paged_adamw_8bit\",  # Using AdamW optimizer from torch\n",
    "    load_best_model_at_end=True, \n",
    "    gradient_accumulation_steps = 8, # reduce vram\n",
    "\n",
    "\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Train model \n",
    "for name, param in model.named_parameters():\n",
    "    if not param.is_contiguous():\n",
    "        param.data = param.data.contiguous()\n",
    "\n",
    "project = \"mt5base-thesis-finetune-\"\n",
    "run_name = 'train-model-dir-round-5'\n",
    "output_dir = \"./\" + project + run_name\n",
    "\n",
    "trainer.train()\n",
    "model.config._name_or_path = output_dir\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "base_model = model.base_model  \n",
    "base_model.save_pretrained(output_dir)  \n",
    "trainer.save_model(output_dir) \n",
    "model.config.save_pretrained(output_dir)  \n",
    "base_model.config.save_pretrained(output_dir)  \n",
    "\n",
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb31072-54b8-4f02-8162-3e20422029f6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-22T13:38:52.687862Z",
     "iopub.status.idle": "2025-03-22T13:38:52.688022Z",
     "shell.execute_reply": "2025-03-22T13:38:52.687940Z",
     "shell.execute_reply.started": "2025-03-22T13:38:52.687940Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"./mt5base-thesis-finetune-train-model-dir-round-5\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e03707-3229-4a58-8312-95171bf985a6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-22T13:38:52.688811Z",
     "iopub.status.idle": "2025-03-22T13:38:52.688987Z",
     "shell.execute_reply": "2025-03-22T13:38:52.688902Z",
     "shell.execute_reply.started": "2025-03-22T13:38:52.688894Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#  ROUGE score calculation\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, num_samples):\n",
    "    model.eval() \n",
    "    preds, refs = [], []\n",
    "    \n",
    "    for i, sample in enumerate(dataset):\n",
    "        if i >= num_samples: \n",
    "            break\n",
    "        \n",
    "        input_text = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n",
    "        reference_summary = tokenizer.decode(\n",
    "            [token for token in sample[\"labels\"] if token != -100], skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        inputs = {\n",
    "            \"input_ids\": sample[\"input_ids\"].unsqueeze(0).to(device),\n",
    "            \"attention_mask\": sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "        }\n",
    "        \n",
    "        # Generate summary with optimized parameters\n",
    "        with torch.no_grad():output_tokens = model.generate(**inputs, max_length=1000, num_beams=5)\n",
    "        \n",
    "        # Decode summary output and append\n",
    "        predicted_summary = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        preds.append(predicted_summary)\n",
    "        refs.append(reference_summary)\n",
    "    \n",
    "    # คำนวณ ROUGE score\n",
    "    rouge_scores = rouge.compute(predictions=preds, references=refs)\n",
    "    \n",
    "    return rouge_scores\n",
    "\n",
    "# ROUGE Score\n",
    "rouge_results = evaluate_model(model, tokenizer, valid_dataset, num_samples=200)\n",
    "\n",
    "print(\"ROUGE Score:\", rouge_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a4bce2-fe55-424b-9e8d-d58dc22f780a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-22T13:38:52.690014Z",
     "iopub.status.idle": "2025-03-22T13:38:52.690249Z",
     "shell.execute_reply": "2025-03-22T13:38:52.690096Z",
     "shell.execute_reply.started": "2025-03-22T13:38:52.690096Z"
    }
   },
   "outputs": [],
   "source": [
    "# real test \n",
    "def summarize_text(text, model, tokenizer, max_input_length=3000, max_output_length=1500):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_input_length)\n",
    "    \n",
    "    # Move tensors to the same device as the model\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=max_output_length,\n",
    "    min_length=200,\n",
    "    num_beams=5,             \n",
    "    do_sample=True,        \n",
    "    length_penalty=1.1,      \n",
    "    no_repeat_ngram_size=3,  \n",
    "    repetition_penalty=1.1,   \n",
    "    top_p=0.95,               \n",
    "    temperature=0.9,\n",
    "    early_stopping=True       \n",
    "    )\n",
    "\n",
    "\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "sample_text = \"\"\"ประเทศไทย- หรือชื่อทางการว่า ราชอาณาจักรไทย เป็นรัฐชาติอันตั้งอยู่บนคาบสมุทรอินโดจีนและมลายู ในภูมิภาคเอเชียตะวันออกเฉียงใต้ มีพรมแดนด้านตะวันออกติดประเทศลาวและประเทศกัมพูชา ทิศใต้เป็นแดนต่อแดนประเทศมาเลเซียและอ่าวไทย ทิศตะวันตกติดทะเลอันดามันและประเทศพม่า และทิศเหนือติดประเทศพม่าและลาว มีแม่น้ำโขงกั้นเป็นบางช่วง ปกครองด้วยระบอบประชาธิปไตยแบบมีรัฐสภา มีศูนย์กลางการบริหารราชการแผ่นดินอยู่ที่กรุงเทพมหานคร และการปกครองส่วนภูมิภาค จัดระเบียบเป็น 76 จังหวัด\n",
    "ประเทศไทยมีขนาดใหญ่เป็นอันดับที่ 50 ของโลก มีเนื้อที่ 513,115 ตารางกิโลเมตร และมีประชากรมากเป็นอันดับที่ 20 ของโลก คือ ประมาณ 66 ล้านคน กับทั้งยังเป็นประเทศอุตสาหกรรมใหม่ โดยมีรายได้หลักจากภาคอุตสาหกรรมและการบริการ ไทยมีแหล่งท่องเที่ยวที่มีชื่อเสียงเป็นอันมาก อาทิ พัทยา, ภูเก็ต, กรุงเทพมหานคร และเชียงใหม่ ซึ่งสร้างรายได้ให้แก่ประเทศ เช่นเดียวกับการส่งออกอันมีส่วนสำคัญในการพัฒนาเศรษฐกิจและด้วยจีดีพีของประเทศ ซึ่งมีมูลค่าราว 334,026 ล้านดอลลาร์สหรัฐ ตามที่ประมาณใน พ.ศ. 2553 เศรษฐกิจของประเทศไทยนับว่าใหญ่เป็นอันดับที่ 30 ของโลก\n",
    "ในอาณาเขตประเทศไทย พบหลักฐานของมนุษย์ซึ่งมีอายุเก่าแก่ที่สุดถึงห้าแสนปี นักประวัติศาสตร์มักถือว่าอาณาจักรสุโขทัยเป็นจุดเริ่มต้นของประวัติศาสตร์ไทย ซึ่งต่อมาตกอยู่ในอิทธิพลของอาณาจักรอยุธยา อันมีความยิ่งใหญ่กว่า และมีการติดต่อกับชาติตะวันตก อาณาจักรอยุธยามีอายุยืนยาว 417 ปีก็เสื่อมอำนาจและล่มสลายไปโดยสิ้นเชิง สมเด็จพระเจ้ากรุงธนบุรีทรงกอบกู้เอกราชและสถาปนาอาณาจักรธนบุรี เหตุการณ์ความวุ่นวายในช่วงปลายอาณาจักร นำไปสู่ยุคสมัยของราชวงศ์จักรีแห่งกรุงรัตนโกสินทร์\n",
    "ช่วงต้นกรุง ประเทศเผชิญภัยคุกคามจากชาติใกล้เคียง แต่หลังรัชสมัยพระบาทสมเด็จพระจอมเกล้าเจ้าอยู่หัวเป็นต้นมา ชาติตะวันตกเริ่มมีอิทธิพลในภูมิภาคเป็นอย่างมาก นำไปสู่การเข้าเป็นภาคีแห่งสนธิสัญญาหลายฉบับ และการเสียดินแดนบางส่วน กระนั้น ไทยก็ยังธำรงตนมิได้เป็นอาณานิคมของชาติใด ๆ ต่อมาจนช่วงสงครามโลกครั้งที่หนึ่ง ไทยได้เข้าร่วมกับฝ่ายสัมพันธมิตร และในปี พ.ศ. 2475 ได้มีการปฏิวัติเปลี่ยนแปลงจากระบอบสมบูรณาญาสิทธิราชมาเป็นประชาธิปไตย และไทยได้เข้ากับฝ่ายอักษะในระหว่างสงครามโลกครั้งที่สอง จนช่วงสงครามเย็น ไทยได้ดำเนินนโยบายเป็นพันธมิตรกับสหรัฐอเมริกา ทหารเข้ามามีบทบาทในการเมืองไทยอย่างมากหลังปฏิวัติสยามอยู่หลายสิบปี กระทั่งมีการตั้งรัฐบาลพลเรือน และเข้าสู่ยุคโลกเสรีในปัจจุบัน\"\"\"\n",
    "print(\"🔹 ข้อความต้นฉบับ: \", sample_text)\n",
    "print(\"🔹 ข้อความสรุป:\", summarize_text(sample_text, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f758f92-cd64-4ad3-8b30-44cad1266e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
